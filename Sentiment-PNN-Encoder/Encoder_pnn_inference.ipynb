{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "main",
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import BertTokenizer\n",
        "from torch.cuda.amp import autocast\n",
        "import numpy as np\n",
        "\n",
        "# PNN Column: MLP for a single task/modality\n",
        "class PNNColumn(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, input_dim)\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.mlp(x)\n",
        "\n",
        "# PNN: Manages multiple columns and lateral connections\n",
        "class PNN(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, device):\n",
        "        super().__init__()\n",
        "        self.columns = nn.ModuleList()  # Store columns for tasks/modalities\n",
        "        self.adapters = nn.ModuleList()  # Lateral connections for each column\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.device = device\n",
        "    \n",
        "    def add_column(self):\n",
        "        # Add a new column for a new task/modality and move to device\n",
        "        column = PNNColumn(self.input_dim, self.hidden_dim).to(self.device)\n",
        "        self.columns.append(column)\n",
        "        # Add adapters for previous columns (if any) and move to device\n",
        "        adapters = nn.ModuleList([\n",
        "            nn.Linear(self.input_dim, self.input_dim).to(self.device) for _ in range(len(self.columns) - 1)\n",
        "        ])\n",
        "        self.adapters.append(adapters)\n",
        "        # Freeze previous columns to prevent forgetting\n",
        "        for i in range(len(self.columns) - 1):\n",
        "            for param in self.columns[i].parameters():\n",
        "                param.requires_grad = False\n",
        "    \n",
        "    def forward(self, x, task_id):\n",
        "        # Compute output for task_id\n",
        "        column_output = self.columns[task_id](x)\n",
        "        # Add lateral connections from previous columns\n",
        "        lateral = 0\n",
        "        for j, adapter in enumerate(self.adapters[task_id]):\n",
        "            lateral += adapter(self.columns[j](x))\n",
        "        return column_output + lateral\n",
        "\n",
        "# Custom Transformer Encoder Layer with PNN\n",
        "class PNNTransformerEncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, nhead, dim_feedforward, dropout=0.1, device='cpu'):\n",
        "        super().__init__()\n",
        "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
        "        self.pnn = PNN(d_model, dim_feedforward, device)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    \n",
        "    def add_column(self):\n",
        "        self.pnn.add_column()\n",
        "    \n",
        "    def forward(self, src, task_id, src_mask=None, src_key_padding_mask=None):\n",
        "        # Self-attention\n",
        "        attn_output, _ = self.self_attn(src, src, src, attn_mask=src_mask, key_padding_mask=src_key_padding_mask)\n",
        "        src = self.norm1(src + self.dropout(attn_output))\n",
        "        # PNN\n",
        "        pnn_output = self.pnn(src, task_id)\n",
        "        src = self.norm2(src + self.dropout(pnn_output))\n",
        "        return src\n",
        "\n",
        "# Encoder-Only Transformer with PNN\n",
        "class TransformerWithPNN(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model=384, nhead=6, num_layers=4, dim_feedforward=1536, num_classes=2, dropout=0.1, device='cpu'):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoder = nn.Parameter(torch.zeros(1, 512, d_model))  # Positional encoding\n",
        "        self.encoder_layers = nn.ModuleList([\n",
        "            PNNTransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout, device) for _ in range(num_layers)\n",
        "        ])\n",
        "        self.classifier = nn.Linear(d_model, num_classes)\n",
        "        self.device = device\n",
        "        self.init_weights()\n",
        "    \n",
        "    def init_weights(self):\n",
        "        nn.init.xavier_uniform_(self.embedding.weight)\n",
        "        nn.init.normal_(self.pos_encoder, std=0.02)\n",
        "    \n",
        "    def add_task(self):\n",
        "        # Add a new column for each encoder layer\n",
        "        for layer in self.encoder_layers:\n",
        "            layer.add_column()\n",
        "    \n",
        "    def forward(self, src, task_id, src_key_padding_mask=None):\n",
        "        # src: (batch, seq_len)\n",
        "        src = self.embedding(src) * torch.sqrt(torch.tensor(self.d_model, dtype=torch.float32))\n",
        "        src = src + self.pos_encoder[:, :src.size(1), :]\n",
        "        src = src.permute(1, 0, 2)  # (seq_len, batch, d_model)\n",
        "        \n",
        "        # Pass through encoder layers\n",
        "        for layer in self.encoder_layers:\n",
        "            src = layer(src, task_id, src_key_padding_mask=src_key_padding_mask)\n",
        "        \n",
        "        # Take [CLS] token (first token) for classification\n",
        "        cls_output = src[0, :, :]  # (batch, d_model)\n",
        "        logits = self.classifier(cls_output)  # (batch, num_classes)\n",
        "        return logits\n",
        "\n",
        "# Inference function\n",
        "def predict_sentiment(model, tokenizer, texts, device, max_length=128, task_id=0):\n",
        "    model.eval()\n",
        "    sentiments = {0: \"Negative\", 1: \"Positive\"}\n",
        "    results = []\n",
        "    \n",
        "    # Handle single string or list of strings\n",
        "    if isinstance(texts, str):\n",
        "        texts = [texts]\n",
        "    \n",
        "    # Tokenize inputs\n",
        "    encodings = tokenizer(\n",
        "        texts,\n",
        "        add_special_tokens=True,\n",
        "        max_length=max_length,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "    input_ids = encodings['input_ids'].to(device)\n",
        "    attention_mask = encodings['attention_mask'].to(device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        with autocast():\n",
        "            logits = model(input_ids, task_id, src_key_padding_mask=(attention_mask == 0))\n",
        "        probs = torch.softmax(logits, dim=1).cpu().numpy()  # Confidence scores\n",
        "        preds = torch.argmax(logits, dim=1).cpu().numpy()  # Predicted classes\n",
        "    \n",
        "    for i, text in enumerate(texts):\n",
        "        sentiment = sentiments[preds[i]]\n",
        "        confidence = probs[i][preds[i]]\n",
        "        results.append({\n",
        "            'text': text,\n",
        "            'sentiment': sentiment,\n",
        "            'confidence': confidence\n",
        "        })\n",
        "    \n",
        "    return results\n",
        "\n",
        "# Main script for inference\n",
        "if __name__ == \"__main__\":\n",
        "    # Hyperparameters (must match training)\n",
        "    VOCAB_SIZE = 30522  # BERT tokenizer vocab size\n",
        "    D_MODEL = 384\n",
        "    NHEAD = 6\n",
        "    NUM_LAYERS = 4\n",
        "    DIM_FEEDFORWARD = 1536\n",
        "    NUM_CLASSES = 2\n",
        "    MAX_LENGTH = 128\n",
        "    MODEL_PATH = r'Path\\transformer_pnn_imdb_rtx4080.pth'\n",
        "    \n",
        "    # Device\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "    \n",
        "    # Load tokenizer\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "    \n",
        "    # Initialize model\n",
        "    model = TransformerWithPNN(\n",
        "        vocab_size=VOCAB_SIZE,\n",
        "        d_model=D_MODEL,\n",
        "        nhead=NHEAD,\n",
        "        num_layers=NUM_LAYERS,\n",
        "        dim_feedforward=DIM_FEEDFORWARD,\n",
        "        num_classes=NUM_CLASSES,\n",
        "        device=device\n",
        "    ).to(device)\n",
        "    \n",
        "    # Add first task (text-based sentiment, matching training)\n",
        "    model.add_task()\n",
        "    \n",
        "    # Load trained weights\n",
        "    try:\n",
        "        model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
        "        print(f\"Loaded model weights from {MODEL_PATH}\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: Model file {MODEL_PATH} not found. Please train the model first.\")\n",
        "        exit(1)\n",
        "    \n",
        "    # Test sentences\n",
        "    test_texts = [\n",
        "        \"This movie was absolutely amazing!\",\n",
        "        \"Terrible film, a complete waste of time.\",\n",
        "        \"The plot was okay, but the acting was mediocre.\",\n",
        "        \"I loved every minute of this masterpiece!\"\n",
        "    ]\n",
        "    \n",
        "    # Perform inference\n",
        "    print(\"\\nRunning inference...\")\n",
        "    results = predict_sentiment(model, tokenizer, test_texts, device, MAX_LENGTH, task_id=0)\n",
        "    \n",
        "    # Display results\n",
        "    for result in results:\n",
        "        print(f\"\\nText: {result['text']}\")\n",
        "        print(f\"Sentiment: {result['sentiment']}\")\n",
        "        print(f\"Confidence: {result['confidence']:.4f}\")\n",
        "    \n",
        "    # Interactive inference (optional)\n",
        "    print(\"\\nEnter your own text for sentiment prediction (or 'quit' to exit):\")\n",
        "    while True:\n",
        "        user_input = input(\"> \")\n",
        "        if user_input.lower() == 'quit':\n",
        "            break\n",
        "        results = predict_sentiment(model, tokenizer, user_input, device, MAX_LENGTH, task_id=0)\n",
        "        for result in results:\n",
        "            print(f\"Text: {result['text']}\")\n",
        "            print(f\"Sentiment: {result['sentiment']}\")\n",
        "            print(f\"Confidence: {result['confidence']:.4f}\")\n"
      ]
    }
  ],
  "metadata": {},
  "nbformat": 4,
  "nbformat_minor": 5
}
